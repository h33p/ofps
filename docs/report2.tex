\documentclass[11pt,english]{report}

\usepackage{bussproofs}
\usepackage{minted}
\setminted{encoding=utf-8}
\usepackage{fontspec}
\setmainfont{FreeSerif}
\setmonofont{FreeMono}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usetikzlibrary{automata,positioning}
\usepackage{amssymb,amsmath}
\usepackage[makeroom]{cancel}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage{unicode-math}
\usepackage{float}
\usepackage{subcaption}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\bibliography{docs/report}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\author{
	Aurimas Bla≈æulionis (2069871)\\
	Alexander Krull\\
	\texttt{axb1440@student.bham.ac.uk}
}

\title{Optical Flow Processing Stack and Use of MPEG Motion Vectors}

\begin {document}
\maketitle

\section*{Abstract}

Tracking camera motion is an important task in many industries, including film, augmented reality and robotics. Detecting whenever there is motion in video is important for various other tasks, such as reducing the amount of storage used by CCTV cameras. In both of these cases, motion is extracted from video and is then processed. One, widely used method to describe motion is called optical flow. However, it is typically computationally expensive to calculate. MPEG's H.264 and other video compression standards coincidentally happen to encode motion in order to minimise new pixel data needed across consecutive frames. The aforementioned codec is often hardware accelerated on commodity hardware, meaning it could be possible to build an efficient optical flow pipeline and perform motion detection and tracking tasks at low cost. However, these optical flow fields are very noisy due to video encoders prioritising storage efficiency over motion accuracy. This project analyses how useful MPEG motion vectors are for motion detection and tracking tasks, compared to other industry standard methods, proposes a general framework for processing optical flow fields dubbed "OFPS", as well as an accompanying app that is able to perform motion detection and tracking tasks alongside a custom 3D renderer that is able to visualise tracking results.

\tableofcontents

\par\noindent\rule{\textwidth}{0.4pt}

\chapter{Introduction}

Camera motion estimation is a task of determining how a camera moves in 3D space over time. Film industry relies on this task being performed accurately in order to overlay computer generated imagery atop filmed video, augmented reality applications attempt to perform the same process, but in realtime, while robotics industry has given this task the name of "visual odometry", and plenty of research has been put in performing it accurately on a live robot, while also extending it to perform dense reconstruction, simultanious localisation and mapping.

Typically, it is done by extracting moving features across frames and using a sufficient number of them to solve parameters of some camera motion model. Two distinct classes of feature extraction methods emerge: feature tracking and optical flow based. Feature tracking sparsely picks out highly contrasting parts of a frame and follows them for as many frames as possible. Meanwhile, optical flow represents apparent motion of objects across 2 consecutive frames and it can either be sparse or dense. An optical flow field is effectively a list of point and vector pairs representing 2D motion at given pixel coordinates. The key difference between these 2 methods is that feature tracking can represent motion with less error across a sequence of frames, while optical flow typically has more information for 2 consecutive frames, at a loss of continuity. Given the sheer amount of pixels needed to be processed, feature extraction step is often more expensive than 3D motion estimation. For instance, MaskFlowNet \cite{zhao2020maskflownet}, a state of the art dense optical flow generation method takes upwards of 300 milliseconds to compute per frame on a laptop GPU, while VOLDOR \cite{voldor} visual odometry system takes just 100 milliseconds to consume the outputted optical flow field and produce accurate visual odometry data.

(Figure: Feature tracking vs. Optical Flow)

Moving Picture Experts Group (MPEG) have designed a number of video encoding standards which heavily compress video files at low perceptual quality loss and they are ubiquitous to the point where most commodity hardware dedicate part of their silicon space for accelerated encoding and decoding of the latest MPEG video codecs - H.264/AVC \cite{h264} and H.265/HEVC \cite{h265}. The way this compression works is that blocks of pixels get moved between frames in accordance to encoded motion vectors and only a small amount of new pixels are encoded to fill in the gaps and make small adjustments to the resulting frame. Essentially, to save space video encoding relies on apparent motion estimation, or in other words - optical flow field computation. Many hardware accelerated H.264 encoders are able to process over 30 frames per second at resolutions of 1080p, thus if one could extract the encoded motion in a H.264 stream, a highly efficient motion estimation pipeline could be built to produce 3D motion data in realtime. However, the primary task of a video encoder is to compress video, not to compute accurate optical flow, thus the resulting motion field is noisy \cite{1334181} and not as dense as one would prefer. As a result, their usability for 3D motion estimation is uncertain.

(Figure: MPEG motion vectors)

This project analyses MPEG motion vectors, compares them against industry standard optical flow computation methods, looks at the ways they can be used in realtime operation and in the process we build a framework alongside an app for performing several motion processing tasks, such as motion detection and estimation.

\section{Project Aims}

The aim of this project is to build an integrated system for performing motion detection and camera tracking using high efficiency MPEG motion vectors. This project will focus on motion vectors created by H.264 video encoders, but will be built in a modular way so that different optical flow, motion detection and tracking methods could be composed. The motion tracking part of the project will be evaluated using synthetic 3D scenes, where precise camera motion is known. Quantitative results will be compared using different optical flow generation methods, as well as different motion tracking methods. The system will also be tested against several real world clips where a camera pans around a relatively stationary point. Evaluation against real-world clips will be less quantitative, but will focus more on whether it "looks good". Motion detection will be tested against a live H.264 stream coming from a Raspberry Pi, emulating CCTV camera conditions. The project aims to package this into a set of modular libraries and an application that provides a user-friendly interface for loading up pre-recorded or live videos and performing these motion detection and tracking tasks.

\section{Related Work}

There are several papers exploring motion tracking using MPEG motion vectors \cite{1334181} \cite{1414440} \cite{825867}. One paper improves upon the previously mentioned techniques and achieves comparitavely high accuracy using simple motion field modeling and RANSAC for outlier removal \cite{almeida}. It is promising, however, due to its method not taking into account camera's field of view, the estimates of motion will only be correct up to a scale factor (thus their analysis only computed model's corellation not error). In addition, the authors' motion models are not accurate at the edges of the screen, thus there seems to be room for improvement (this point is related to not knowing the camera's field of view). Furthermore, the paper does not go into detail how exactly they implemented RANSAC - the inlier/outlier criteria is not specified. Overall, this paper is a good implementation candidate, but will require further development to complete it.

Broader, there is plenty of work in multiple view geometry. Several industry standard tools work on sequences of tracked 2D points, including Blender's libmv \cite{libmv} and OpenCV \cite{opencv5calib}. There also exists a paper improving the algorithms employed by the libraries, and works with just 5 data points \cite{1211470}. At the base case, the underlying algorithms attempt to estimate a matrix that relates points between images, known as fundamental matrix. There is an implicit problem of scale ambiguity \cite{hartley_zisserman_2004} with these methods - it is difficult to have consistent positional scale throughout time, because computed matrices produce translation values accurate only up to a scale factor. The aforementioned libraries have built-in algorithms for solving the problem for a sequence of frames, and they are proven to work well, but points need to be tracked throughout the sequence, which makes it unsuitable for optical flow.

There is recent work on creating visual odometry from dense optical flow fields \cite{voldor}, which is typically combined with a dense optical flow estimator, such as MaskFlowNet \cite{zhao2020maskflownet}. VOLDOR \cite{voldor} is a novel approach that models a probabilistic model of the optical flow field, iteratively estimates a depth map from a series of moving frames and outputs camera poses. Unfortunately, VOLDOR is not able to build a depth map with MPEG motion vectors due to their noisiness.

(Figure: MaskFlowNet vs. MPEG)

Neither of the mentioned work solves the our problem completely. Thus, we believe there is a gap in current research and development, due to no system existing for doing realtime absolute motion tracking using MPEG. The papers either sacrifice realtime performance for accuracy, or are underspecified and too unconstrained. Other work does not suit optical flow very well, and by extension is not perfectly suitable for our domain. Thus, we will attempt to combine several of these methods, propose potential improvements and analyse what works best.

\chapter{Background}

This chapter introduces several concepts related to the project, most notably lower level details of MPEG compression and basics of epipolar geometry.

\section{MPEG Video}

Motion Picture Experts Group has developed a number of video encoding standards over the years. The first one being MPEG-1 and was released in 1993. Fundamentally, it is very similar to currently one of the most widely used H.264/AVC/MPEG-4 Part 10 standard, which was published in 2004. This section will discuss features relevant to the project that exist in H.264 and older versions, but will not cover H.265, due to large differences in the standard.

\subsection{Encoding}

Various video streaming websites serve H.264 encoded video. YouTube recommends a 1080p video to be encoded at the bitrate of 8 megabits per second \cite{youtube}. For context, a single $1920 \times 1080$ RGB frame is nearly 8 megabytes of size, while a single JPEG will average between 100 and 300 kilobytes in size. For a 30 FPS video, YouTube asks each frame to average 34 kilobytes in size - that is $1/470$th of a raw frame, and around $1/5$th of a JPEG. Yet, a video with such high compression will look sufficiently high quality for a viewer not to notice any artifacts. The reason this is possible is due to intra-frame compression.

An MPEG video consists of several sparsely placed intra-frames, known as \textbf{I-frames} and series of predicted frames (\textbf{P-frames} and \textbf{B-frames}). Intra-frame is effectively a JPEG image, while a predicted frame contains changes since the previous frame. The only difference between P and B frames is that P frames only look at a frame behind, while B frames predict bi-directionally between the previous frame and the I-frame that comes right afterwards. A single intra-frame and all subsequent predicted frames form a group of pictures, known as GoP.

(Diagram showing a GoP and the different frames within)

Predicted frames is how MPEG video is able to achieve such a high compression ratio. In AVC, each frame contains a number of transform and prediction macroblocks. Transform blocks are $4 \times 4$ or $8 \times 8$ sized chunks that describe new pixel values at part of the frame in a way where chroma values are subsampled to save space. Prediction blocks are $4 \times 4$ to $16 \times 16$ pixel sized chunks that get shifted across frames and motion of each macroblock is defined by a motion vector - pixel data is reused and only moved from one location to the next. The motion of prediction blocks will be needed by the project and an extraction module will be built.

\subsubsection{"Datamoshing"}

An interesting experiment can be done to visualise how MPEG works by intentionally corrupting the encoded stream. In non-scientific literature this process is known as datamoshing. If a video contains just a single GoP, it is possible to replace the only I-frame in the video with a different image and visualy see what changes are encoded in the predicted frames throughout the entire clip. Most notably, high contrast areas are likely to be moved in accordance to camera motion, while low contrast areas are likely to blur themselves out due to the noisiness of the encoded motion vectors.

The project provides a bash script to perform datamoshing, and it is available at scripts/mosh.sh.

\section{Two-View Geometry}

Two-view geometry, also known as epipolar geometry concerns itself about the relationship of 2 views given 2D point correspondances. From a single view it is impossible to determine how far away a point in the image is - it only defines an epipolar line, however, the second view constraints the point and allows us to triangulate its position on the line. Often times, pose of the views is unknown either. Thus, to perform triangulation, relationship between the 2 views needs to be calculated from a number of 2D point correspondances in the form of a fundamental matrix.

(Figure: epipolar geometry)

\subsection{The fundamental matrix}

The fundamental matrix is an algebraic representation of epipolar geometry. It is a $3 \times 3$ matrix of rank 2. If a point in 3-space $X$ is imaged as $x$ in the first view, and $x'$ in the second, then the image points satisfy the relation $x'^T \mathbf{F} x = 0$ \cite{hartley_zisserman_2004}. The fundamental matrix has 7 degrees of freedom - it depends on both view pose changes, and intrinsic camera parameters. Several industry standard libraries implement algorithms for robust fundamental matrix estimation, such as Blender's libmv and OpenCV. There is a recent paper describing robust estimation from only 5 data points \cite{8578130}. Once a fundamental matrix is known, and given intrinsic camera parameters, it is possible to compute 3D positions of the 2D point correspondances, through the process of triangulation.

\subsection{The essential matrix}

If camera intrinsics are known (camera is calibrated), it is possible to use them to constrain the freedom of the fundamental matrix, and compute it more accurately, with fewer data points. The essential matrix is the the specialisation of the fundamental matrix to the case of normalised image coordinates \cite{hartley_zisserman_2004}. The essential matrix has fewer degrees of freedom - only 5. Other parameters are constrained by requiring knowlege of intrinsic camera parameters. OpenCV has an implementation solving the essential matrix using D. Nister's efficient 5 point algorithm \cite{1211470}.

\chapter{Methods and Implementation}

This project contains a lot of moving parts across different disciplines: setting up hardware, software architecture design and development, paper implementation and improvements, visualisation work. Initially, the project focused around motion vector extraction from H.264 streams and involved testing of its functionality. Later, it built several 3D motion estimation modules, starting from Almeida, then attempting to use libmv and OpenCV. Due to imperfect results, a different optical flow processing task was found - motion detection and thus a module was built for that purpose. Finally, once the library and accompanying modules proved to work sufficiently well, the core API was reworked, dynamic loading was built-in and a user friendly application was built, which required further development, such as a 3D renderer, to present the results intuitively. The sections below describe the progress in the previously given order.

Most of the code was written in Rust. It is a systems programming language that maximises performance without sacroficing safety, language abstractions and modularity. Given the project is focused around performance, the language was a clear choice. Standard Rust toolchain is employed, the current compiler version being 1.59.0. The project is compiled using standard cargo utility. The only non-standard part is the libmv module and its nuances will be explained in the dedicated section.

\section{Optical Flow Processing Stack}

OFPS is the core library of the project. It contains everything needed for the user to process optical flow. Primary focus was developing an architecture that was both trivial to use and could scale well, i.e. able to support any future methods of motion extraction, detection and estimation. Thus, the library did undergo iterative development with small changes to the API as time passed. Nalgebra was chosen as the primary numpy-like math crate (library in Rust terms) due to its flexibility, simplicity to use and proven time record.

\subsection{Pinhole camera}

We define a standard pinhole camera model. A key design decision was to make resolution information unnecessary when processing optical flow, thus pixel coordinates are normalised in $[0; 1]$ range and the principal point is at $(0.5; 0.5)$ point. Effectively all images are stretched to have square aspect ratio. This is a similar concept to anamorphic lenses - film industry often uses special lenses that have a higher horizontal field of view by bending the incoming light more horizontally than vertically. The frame then needs to be stretched out in post-production. Our pinhole camera model essentially does the same - vertical and horizontal fields of view are independent, which is equivalent to having differing focal lengths. Any processing algorithm then uses these intrinsic parameters instead of video resolution to process the motion vectors.

\subsection{Interface design}

Three generic interfaces, known as traits in Rust, were developed to provide a simple yet flexible standard for motion decoding, estimation and detection. In addition, one general interface for configuring underlying properties of the plugins was defined.

\subsubsection{Decoder}

A Decoder either extracts precomputed motion from an input stream, or calculates it from image data. The input stream is not visible to the user - it is stored within the decoder. Initially, Decoder was designed to output a MotionField, which is a dense fixed-size optical flow structure. Producing fixed-size fields may result in predictable performance characteristics, however, it was noticed that sampling raw motion vectors into fixed resolution structure lowered accuracy of the estimators consuming the field, thus the motion is now being outputted as a list of motion vectors - pairs of points representing the location of motion on screen and vectors representing the motion. To make code resolution-independent, screen locations and motion vectors are normalised in $[0; 1]$ range. The interface was designed to support cases with no motion information present (e.g. I-frames) and it supports optional output of RGB frames. It is optional due to some decoders potentially not having any access to raw frame data, e.g. precomputed motion fields and specialised video decoders that only extract motion vectors. There are also several optional fields to provide timing and video aspect ratio information. Overall, the trait does not impose any unnecessary requirements that would lower the performance of the decoding.

\subsubsection{Estimator}

An Estimator accepts a list of normalised motion vectors alongside pinhole camera and outputs estimated rotation and translation of the camera. The estimator is responsible resolving translation scale ambiguity and thus is not guaranteed to be stateless - motion estimator may rely on previous inputs to produce more accurate estimates of transformation.

\subsubsection{Detector}

A Detector takes a list of normalised motion vectors and outputs whether there is motion or not. It is optionally able to return a dense motion field describing the motion, if there is any, however, it is not a required functionality.

\subsubsection{Properties}

Properties trait is implemented by all plugins. It is a trait that provides a list of string, boolean, floating point and integer properties that are available to be configured when using unknown decoder, estimator or detector. This trait is useful when paired with the plugin system, and is used by the OFPS Suite application to provide further user control.

\subsection{Plugin System}

To make the project modular we added dynamically loadable plugin system. A PluginStore object searches the current working directory, executable's directory, and several other predefined directories for shared libraries that implement OFPS traits. All discovered libraries are stored and get loaded upon request. A library may define that it implements an OFPS trait by using the define\_descriptor macro. The macro helps the library author create a special header that gets parsed by OFPS plugin loader. The header contains the name of the plugin, initialisation function and several safety fields.

We had to add certain restrictions to ensure safety when loading these plugins. Since Rust is an evolving language, it does not yet define a stable Application Binary Interface (ABI) \cite{abi-ip} and code compiled on different compilers may be incompatible in subtle ways. This can lead to runtime crashes, or worse, unintended changes in program's control flow. One way to approach this problem would be to utilise CGlue opaque objects \cite{cglue}, which are essentially thinly wrapped objects in stable C ABI. It would provide the most safety possible, however, nalgebra and other extensively used crates use C-incompatible structures, and would require to be converted to C-compatible types when passing interface boundaries. Imposing such requirements would make OFPS too verbose to use and thus an alternative approach was taken. For the following approach we rely on present-day fact that language ABI can only change either when compiler gets updated, or when a special unstable -Z randomize-layout \cite{randomize-layout} flag is used. Concluding that on stable Rust toolchain ABI never changes, we added compiler version checking to the plugin loader. In addition, a special API\_VERSION constant was added to prevent loading API-incompatible code. In the end, we get a stable and modular dynamically loadable plugin system in a programming language that does not provide such facilities.

\section{Raspberry Pi setup}

A 8GB Raspberry Pi 4 was acquired with a XX camera for the purposes of live project testing. Standard Raspberry Pi OS was installed and only video RAM was increased. The onboard Broadcom BCM2711 system-on-chip contains a H.264 hardware encoder, capable of processing 1080p frames at 30 frames per second. A script was set up to start a program that encodes live video from a camera and outputs the resulting packets to a remote client through a TCP server that can be accessed on local network. To increase the number of P-frames we customise the GoP size to 240 frames, meaning there is typically just a single I-frame every 8 seconds.

\section{Optical flow field decoders}

Several optical flow generation methods were developed. They are split into separate crates and provide a structure that implements the Decoder trait. These crates can be compiled independently into shared libraries that could then be loaded by OFPS plugin system. This section describes each of the built-in decoders.

\subsection{AV}

AV decoder has its crate named mvec-av, and is responsible for decoding motion vectors using LibAV and FFMPEG. It supports H.264 decoding and extraction of motion vectors. We had to set up a custom input stream for LibAV to be able to process any Rust stream. The library does not support only extracting the motion vectors - it will always perform full H.264 decoding, thus performance may not be optimal. On the other hand, AV decoder is always able to output RGB frame data, which aids in visualisation and the results show it still is sufficiently performant.

\subsection{OpenCV Farneback}

OpenCV decoder has its crate named mvec-cv, and is responsible for computing optical flow using Gunnar-Farneback algorithm \cite{Farnebck2003TwoFrameME}. The computed flow is then outputted to the user. Gunnar-Farneback algorithm calculates a semi-dense optical flow, meaning most of the pixels contain motion information. If the area is of very low contrast, or there is no motion, that pixel will have have motion value equal to $\overrightarrow{0}$. Thus, there is a trade-off between returning all zero vectors, which could potentially skew the results in motion, or no zero vectors, which could skew the results when there is no camera motion. To solve this, a mask is applied to only output motion vectors in high contrast areas. The mask is computed by using sobel filter that is then thresholded and dilated. This proved to work well in both scenarios.

(Figure: motion vector mask)

\section{Motion estimation}

Several 3D motion estimation modules were developed. They are split into separate crates and each of them contains a structure that implements the Estimator trait. This section describes every estimator that was created.

\subsection{Modified Almeida estimator}

Almeida et al. proposed a method that finds the best-fit affine model to estimate camera motion by using least squares method \cite{almeida}. This method only produces rotation and zoom information - no 3D position changes. Since the Estimator trait requires prior knowlege of camera intrinsics, we will remove computation of lens zoom. However, Estimator's rotation output must be absolute and the proposed algorithm does not produce absolute values. We solve this issue by modeling our own panning, tilting and rolling prototypes, that are computed with camera intrinsics. The prototypes are essentially discrete derivatives of rotation by some angle $\epsilon$. Initially we modelled 0.1 degree rotations, but after developing iterative optimisation step the starting value was raised to 30 degrees.

\subsubsection{RANSAC}

Authors used RANdom SAmple Consensus (RANSAC) \cite{FISCHLER1987726} algorithm to make estimations more robust by filtering outliers. The algorithm computes model parameters from a minimal subset of randomly picked data samples and then compares how well the model fits other samples. The samples that the model fits well are called inliers. The process repeats for a number of iterations and the model with the largest inlier set is returned.

The metric used to check whether a sample is an inlier or not depends on the problem being solved and the paper does not specify it. In simple problems, such as line fitting, euclidean distance is often used as an evaluation metric. Given a point $p$ and its estimate $h$, checking if the point is an inlier can be done by checking whether $||p - h|| \leq c$ holds true, where $c$ is a constant threshold. Due to projective distortion, this does not work for our problem. With wide camera view, apparent motion at the center of the frame does not match the motion at the edges of the screen - the latter points have bigger apparent motion than ones at the center. Thus, we had to develop our own metric that works sufficiently well for our problem.

Given a pixel in the view at angle $\alpha$ from the center of view, rotating the camera towards that point by small angle $\theta$ will produce apparent motion that is higher than the motion at the center of view by the factor of $\frac{1}{cos(\alpha)}$. Thus, given a point at angle $\alpha$, its motion vector $\overrightarrow{v}$, model estimate $\overrightarrow{h}$ and theshold $c$, determening whether a sample is an inlier or an outlier can be done by checking whether $||\overrightarrow{h} - \overrightarrow{v}|| * cos(\alpha) \leq c$ holds true, and this is the metric we used for measuring inliers.

(Figure: screen distortion at the edges)

\subsubsection{Iterative optimisation}

We further improved the accuracy of the final estimate by iteratively applying the least squares fitting method until convergence. This is a process similar to gradient descent, which is known in machine learning field. Initially, we start with prototypes that are modeled with 30 degree rotation $\epsilon$. After computing the initial model, we then perform post-optimisation on the inlier set. Every iteration we lower the the angle of rotation in the primitives, compute the model and usually take a fraction $\alpha$ of it to prevent overshooting (only the final iteration of the algorithm adds entirety of the model). The rotation $r_i$ is applied on the inlier set - starting points get moved by the estimate and motion vectors get subtracted by it. $r_i$ is then applied on cumulative rotation: $r = r * r_i$. In the end, $r = r_0 * r_1 * \cdots * r_n$.

(Figure: optimisation steps on the inlier set)


\subsection{libmv estimator}

Patching libmv to compile

Env sourcing

Rust wrapper

Translation triangulation

Three view geometry

\subsection{multiview estimator}

OpenCV find\_essential\_mat

Same triangulation/problems

\section{Motion detection}

Split into chunks

Threshold vectors, put into chunks (average)

Find largest island

Check its size

\section{wimrend 3D renderer}

Flexibility/stand-alonineness

WGPU

Renderer API

Performance (instancing, mesh management)

Custom render pass (for egui)

\section{OFPS Suite}

Simplicity to add apps.

Reusability

\subsection{Motion Detection}

Loading decoder

Loading detector

Setting parameters

Exporting timestamps

Profiling

Multithreaded architecture

\subsection{Motion Tracking}

Loading decoder

Loading estimators

Setting parameters

Ground truth comparison

Profiling

Multithreaded architecture

\chapter{Results}

History: point tracking

\section{Performance}

\section{Accuracy}

\section{Application}

\chapter{Conclusion}

\section{Future Work}

\subsection{HEVC Motion Extraction}

High Efficiency Video Coding, also known as H.265 or MPEG-H Part 2, was published in 2013, and contains a lot of changes compared to previous standards. For starters, the I-frames are using a new image format, known as High Efficiency Image Format, or HEIF in short. Furthermore, HEVC does not use fixed size macroblocks - it instead uses Coding Tree Units (CTUs) that vary in size between $16 \times 16$ and $64 \times 64$ pixels \cite{h265}. 
HEVC has the potential to contain more accurate motion information, thus it would be interesting to write a custom decoding module for H.265 streams.

\printbibliography

\end{document}
