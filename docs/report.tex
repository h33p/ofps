\documentclass[11pt,english]{report}

\usepackage{bussproofs}
\usepackage{minted}
\setminted{encoding=utf-8}
\usepackage{fontspec}
\setmainfont{FreeSerif}
\setmonofont{FreeMono}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usetikzlibrary{automata,positioning}
\usepackage{amssymb,amsmath}
\usepackage[makeroom]{cancel}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage{unicode-math}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\author{
	Aurimas Bla≈æulionis (2069871)\\
	Alexander Krull\\
	\texttt{axb1440@student.bham.ac.uk}
}

\title{Optical Flow Processing Stack and Use of MPEG Motion Vectors}

\begin {document}
\maketitle

\section*{Abstract}

Tracking camera motion is an important task in many industries, including film, augmented reality and robotics. Detecting whenever there is motion in video is important for various other tasks, such as reducing the amount of storage used by CCTV cameras. In both of these cases, motion is extracted from video and is then processed. One, widely used method to describe motion is called optical flow. However, it is typically computationally expensive to calculate. MPEG's H.264 and other video compression standards coincidentally happen to encode motion in order to minimise new pixel data needed across consecutive frames. The aforementioned codec is often hardware accelerated on commodity hardware, meaning it could be possible to build an efficient optical flow pipeline and perform motion detection and tracking tasks at low cost. However, these optical flow fields are very noisy due to video encoders prioritising storage efficiency over motion accuracy. This project analyses how useful MPEG motion vectors are for motion detection and tracking tasks, compared to other industry standard methods, proposes a general framework for processing optical flow fields dubbed "OFPS", as well as an accompanying app that is able to perform motion detection and tracking tasks alongside a custom 3D renderer that is able to visualise tracking results.

\tableofcontents

\par\noindent\rule{\textwidth}{0.4pt}

\chapter{Introduction}

Camera motion tracking is a task of determining how a camera moves in 3D space over time. Film industry relies on this task being performed accurately in order to overlay computer generated imagery atop filmed video, augmented reality applications attempt to perform the same process, but in realtime, while robotics industry has given this task the name of "visual odometry", and a lot of research has been put in performing it accurately on a live robot, while also extending it to perform dense reconstruction, simultanious localisation and mapping.

Typically, it is done by extracting moving features across frames and using a sufficient number of them to solve parameters of some camera motion model. Two distinct classes of feature extraction methods emerge: feature tracking and optical flow based. Feature tracking sparsely picks out highly contrasting parts of a frame and follows them for as many frames as possible. Meanwhile, optical flow represents apparent motion of objects across 2 consecutive frames and it can either be sparse or dense. An optical flow field is effectively a list of point and vector pairs representing 2D motion at given pixel coordinates. The key difference between these 2 methods is that feature tracking can represent motion with less error across a sequence of frames, while optical flow typically has more information for 2 consecutive frames, at a loss of continuity. Given the sheer amount of pixels needed to be processed, feature extraction step is often more expensive than 3D motion estimation. For instance, MaskFlowNet, a state of the art dense optical flow generation method takes upwards of 300 milliseconds to compute per frame on a laptop GPU, while VOLDOR visual odometry system takes just 100 milliseconds to consume the outputted optical flow field and produce accurate visual odometry data.

Moving Picture Experts Group (MPEG) have designed a number of video encoding standards which heavily compress video files at low perceptual quality loss and they are ubiquitous to the point where most commodity hardware dedicate part of their silicon space for accelerated encoding and decoding of the latest MPEG video codecs - H.264/AVC and H.265/HEVC. The way this compression works is that blocks of pixels get moved between frames and only a small amount of new pixels are encoded to fill in the gaps and make small adjustments to the resulting frame. Essentially, to save space video encoding relies on apparent motion estimation, or in other words - optical flow field computation. Many hardware accelerated H.264 encoders are able to process over 60 frames per second at resolutions of 1080p, thus if one could extract the encoded motion in a H.264 stream, a highly efficient motion estimation pipeline could be built to produce 3D motion data in realtime. However, the primary task of a video encoder is to compress video, not to compute accurate optical flow, thus the resulting motion field is noisy and not as dense as one would prefer. As a result, their usability for 3D motion estimation is uncertain.

This project analyses MPEG motion vectors, compares them against industry standard optical flow computation methods, looks at the ways they can be used in realtime operation and in the process we build a framework alongside an app for performing several motion processing tasks, such as motion detection and camera tracking.

\section{Project Aims}

The aim of this project is to build an integrated system for performing motion detection and camera tracking using high efficiency MPEG motion vectors. This project will focus on motion vectors created by H.264 video encoders, but will be built in a modular way so that different optical flow, motion detection and tracking methods could be composed. The motion tracking part of the project will be evaluated using synthetic 3D scenes, where precise camera motion is known. Quantitative results will be compared using different optical flow generation methods, as well as different motion tracking methods. The system will also be tested against several real world clips where a camera pans around a relatively stationary point. Evaluation against real-world clips will be less quantitative, but will focus more on whether it "looks good". Motion detection will be tested against a live H.264 stream coming from a Raspberry Pi, emulating CCTV camera conditions. The project aims to package this into a set of modular libraries and an application that provides a user-friendly interface for loading up pre-recorded or live videos and performing these motion detection and tracking tasks.

\section{Related Work}

There are several papers exploring motion tracking using MPEG motion vectors. One paper improves upon the previously mentioned techniques and achieves comparitavely high accuracy using simple motion field modeling and RANSAC for outlier removal. It is promising, however, due to the method not taking into account camera's field of view, the estimates of motion will only be correct up to a scale factor. In addition, the authors' motion models are not accurate at the edges of the screen, thus there seems to be room for improvement (this point is related to not knowing the camera's field of view). Furthermore, the paper does not go into detail how exactly they implemented RANSAC - the inlier/outlier criteria is not specified.

Broader, there is plenty of work in multiple view geometry. Several industry standard tools work on sequences of tracked 2D points, including Blender's libmv and OpenCV. There also exists a paper improving the algorithms employed by the libraries, and works with just 5 data points. At the base case, the underlying algorithms attempt to estimate a matrix that relates points between images, known as fundamental matrix. There is a big issue of scale with these methods - it is difficult to have consistent positional scale throughout time, because computed matrices produce translation values accurate only up to a scale factor. The aforementioned libraries have built-in algorithms for solving the problem for a sequence of frames, and they are proven to work well, but points need to be tracked throughout the sequence, which makes it unsuitable for optical flow.

There is recent work on creating visual odometry from dense optical flow fields, which is typically combined with a dense optical flow estimator, such as MaskFlowNet. VOLDOR is a novel approach that models a probabilistic model of the optical flow field, iteratively estimates a depth map from a series of moving frames and outputs camera poses. Unfortunately, VOLDOR is not able to build a depth map with MPEG motion vectors due to their noisiness.

Neither of the mentioned work solves the our problem completely. Thus, we believe there is a gap in current research and development, due to no system existing for doing realtime absolute motion tracking using MPEG. The papers either sacrifice realtime performance for accuracy, or are underspecified and too unconstrained. Other work does not suit optical flow very well, and by extension is not perfectly suitable for our domain.

\chapter{Background}



\section{MPEG Video}

Motion Picture Experts Group has developed a number of video encoding standards over the years. The first one being MPEG-1 and was released in 1993. Fundamentally, it is very similar to currently one of the most widely used H.264/AVC/MPEG-4 Part 10 standard, which was published in 2004. This section will discuss features relevant to the project that exist in H.264 and older versions, but will not cover H.265, due to large differences in the standard.

\subsection{Encoding}

Various video streaming websites serve H.264 encoded video. YouTube recommends a 1080p video to be encoded at the bitrate of 8 megabits per second. For context, a single $1920 \times 1080$ RGB frame is nearly 8 megabytes of size, while a single JPEG will average between 100 and 300 kilobytes in size. For a 30 FPS video, YouTube asks each frame to average 34 kilobytes in size - that is $1/470$th of a raw frame, and around $1/5$th of a JPEG. Yet, a video with such high compression will look sufficiently high quality for a viewer not to notice any artifacts. The reason this is possible is due to intra-frame compression.

An MPEG video consists of several sparsely placed intra-frames, known as I-frames and series of predicted frames (P-frames and optionally a B-frame at the end). Intra-frame is effectively a JPEG image, while a predicted frame contains changes since the previous frame. The only difference between P and B frames is that P frames only look at a frame behind, while B frames predict bi-directionally between the previous frame and the I-frame that comes right afterwards. A single intra-frame and all subsequent predicted frames form a group of pictures, known as GOP.

Predicted frames is how MPEG video is able to achieve such a high compression ratio. In AVC, each frame contains a number of transform and prediction macroblocks. Transform blocks are $4 \times 4$ or $8 \times 8$ sized chunks that describe new pixel values at part of the frame in a way where chroma values are subsampled to save space. Prediction blocks are $4 \times 4$ to $16 \times 16$ pixel sized chunks that get shifted across frames and motion of each macroblock is defined by a motion vector - pixel data is reused and only moved from one location to the next. The motion of prediction blocks will be needed by the project and an extraction module will be built.

\subsubsection{"Datamoshing"}

An interesting experiment can be done to visualise how MPEG works by intentionally corrupting the encoded stream. In non-scientific literature this process is known as datamoshing. If a video contains just a single GOP, it is possible to replace the only I-frame in the video with a different image and visualy see what changes are encoded in the predicted frames throughout the entire clip. Most notably, high contrast areas are likely to be moved in accordance to camera motion, while low contrast areas are likely to blur themselves out due to the noisiness of the encoded motion vectors.

\section{Multiple View Geometry}

\subsection{Essential matrix}

\subsection{Fundamental matrix}

\chapter{Methods and Implementation}

\section{Raspberry Pi setup}

\section{Motion field generation}

\subsection{AV decoder}

\subsection{OpenCV Farneback}

\section{Motion detection}

\section{Motion estimation}

\subsection{Modified Almeida's estimator}

\subsection{libmv estimator}

\subsection{multiview estimator}

\section{Optical Flow Processing Stack}

\section{wimrend 3D renderer}

\section{OFPS Suite}

\subsection{Motion Detection}

\subsection{Motion Tracking}

\chapter{Results}

\section{Performance}

\section{Accuracy}

\section{Application}

\chapter{Conclusion}

\section{Future Work}

\subsection{HEVC Motion Extraction}

High Efficiency Video Coding, also known as H.265 or MPEG-H Part 2, was published in 2013, and contains a lot of changes compared to previous standards. For starters, the I-frames are using a new image format, known as High Efficiency Image Format, or HEIF in short. Furthermore, HEVC does not use fixed size macroblocks - it instead uses Coding Tree Units (CTUs) that vary in size between $16 \times 16$ and $64 \times 64$ pixels. 
HEVC has the potential to contain more accurate motion information, thus it would be interesting to write a custom decoding module for H.265 streams.

\end{document}
